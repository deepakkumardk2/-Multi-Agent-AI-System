{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1q5Tnu7zLJuy0T0JlEDPL7YimvYw2LpgN",
      "authorship_tag": "ABX9TyPEH7VkhwgYw22M7x1EYrjQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakkumardk2/-Multi-Agent-AI-System/blob/main/Zero_Shot_ECG_Classification_with_Multimodal_Learning_and_Test_time_Clinical_Knowledge_Enhancement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "iItYHCYzSPfX",
        "outputId": "4317f76b-caf0-4e5d-ca5a-f0d70ce74d32"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wfdb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-229df0d8ca9f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wfdb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import wfdb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, accuracy_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "import warnings\n",
        "import json\n",
        "import random\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Specify the path of your database\n",
        "def setup_dataset_paths():\n",
        "    \"\"\"\n",
        "    Set up and validate the dataset path from Google Drive\n",
        "    \"\"\"\n",
        "    print(\"Setting up dataset paths...\")\n",
        "\n",
        "    # Mount Google Drive if in Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        base_path = '/content/drive/MyDrive'\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except:\n",
        "        base_path = ''\n",
        "        print(\"Running in local environment.\")\n",
        "\n",
        "    # Define the dataset path (using the provided link)\n",
        "    dataset_zip_path = os.path.join(base_path, 'mimic-iv-ecg-diagnostic-electrocardiogram-matched-subset-1.0.zip')\n",
        "    dataset_path = os.path.join(base_path, 'mimic-iv-ecg')\n",
        "\n",
        "    # Create directory for processed data\n",
        "    processed_data_dir = os.path.join(base_path, 'processed_ecg_data')\n",
        "    os.makedirs(processed_data_dir, exist_ok=True)\n",
        "\n",
        "    return {\n",
        "        'base_path': base_path,\n",
        "        'dataset_zip_path': dataset_zip_path,\n",
        "        'dataset_path': dataset_path,\n",
        "        'processed_data_dir': processed_data_dir\n",
        "    }\n",
        "\n",
        "# Run a check if all the files exist in the specified path\n",
        "def check_files_exist(paths):\n",
        "    \"\"\"\n",
        "    Verify all required files exist and extract if needed\n",
        "    \"\"\"\n",
        "    print(\"Checking if all files exist...\")\n",
        "\n",
        "    # Check if dataset zip file exists\n",
        "    if not os.path.exists(paths['dataset_zip_path']):\n",
        "        print(f\"Dataset zip file not found at {paths['dataset_zip_path']}\")\n",
        "        print(\"Please upload the dataset to your Google Drive using the provided link.\")\n",
        "        return False\n",
        "\n",
        "    # Extract dataset if needed\n",
        "    if not os.path.exists(paths['dataset_path']):\n",
        "        print(f\"Extracting dataset to {paths['dataset_path']}...\")\n",
        "        import zipfile\n",
        "        os.makedirs(paths['dataset_path'], exist_ok=True)\n",
        "        with zipfile.ZipFile(paths['dataset_zip_path'], 'r') as zip_ref:\n",
        "            zip_ref.extractall(paths['dataset_path'])\n",
        "        print(\"Dataset extracted successfully.\")\n",
        "\n",
        "    # Check if key files exist\n",
        "    required_files = [\n",
        "        'record_csv.csv',\n",
        "        'report_csv.csv'\n",
        "    ]\n",
        "\n",
        "    for file in required_files:\n",
        "        file_path = os.path.join(paths['dataset_path'], file)\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Required file {file} not found at {file_path}\")\n",
        "            return False\n",
        "\n",
        "    print(\"All required files found.\")\n",
        "    return True\n",
        "\n",
        "# Load record_csv and report_csv\n",
        "def load_csv_files(paths):\n",
        "    \"\"\"\n",
        "    Load the ECG records and reports CSV files\n",
        "    \"\"\"\n",
        "    print(\"Loading CSV files...\")\n",
        "\n",
        "    record_csv_path = os.path.join(paths['dataset_path'], 'record_csv.csv')\n",
        "    report_csv_path = os.path.join(paths['dataset_path'], 'report_csv.csv')\n",
        "\n",
        "    record_df = pd.read_csv(record_csv_path)\n",
        "    report_df = pd.read_csv(report_csv_path)\n",
        "\n",
        "    print(f\"Loaded {len(record_df)} records and {len(report_df)} reports.\")\n",
        "    return record_df, report_df\n",
        "\n",
        "# Initialise the dataset object and check for validation\n",
        "class ECGDataset:\n",
        "    def __init__(self, record_df, report_df, paths):\n",
        "        \"\"\"\n",
        "        Initialize the dataset object and validate data\n",
        "        \"\"\"\n",
        "        self.record_df = record_df\n",
        "        self.report_df = report_df\n",
        "        self.paths = paths\n",
        "\n",
        "        # Validate data\n",
        "        self.validate_data()\n",
        "\n",
        "    def validate_data(self):\n",
        "        \"\"\"\n",
        "        Perform basic validation on the dataset\n",
        "        \"\"\"\n",
        "        print(\"Validating dataset...\")\n",
        "\n",
        "        # Check if records and reports have matching IDs\n",
        "        record_ids = set(self.record_df['subject_id'].astype(str) + '_' + self.record_df['study_id'].astype(str))\n",
        "        report_ids = set(self.report_df['subject_id'].astype(str) + '_' + self.report_df['study_id'].astype(str))\n",
        "\n",
        "        common_ids = record_ids.intersection(report_ids)\n",
        "\n",
        "        print(f\"Found {len(common_ids)} matching records and reports out of {len(record_ids)} records and {len(report_ids)} reports.\")\n",
        "\n",
        "        # Check if there are any null values in important columns\n",
        "        record_nulls = self.record_df[['subject_id', 'study_id']].isnull().sum().sum()\n",
        "        report_nulls = self.report_df[['subject_id', 'study_id', 'text']].isnull().sum().sum()\n",
        "\n",
        "        if record_nulls > 0 or report_nulls > 0:\n",
        "            print(f\"Warning: Found {record_nulls} null values in record_df and {report_nulls} null values in report_df.\")\n",
        "        else:\n",
        "            print(\"No null values found in key columns.\")\n",
        "\n",
        "# Process report and concatenate them\n",
        "def process_reports(dataset):\n",
        "    \"\"\"\n",
        "    Process and clean the report text\n",
        "    \"\"\"\n",
        "    print(\"Processing reports...\")\n",
        "\n",
        "    # Clean report text\n",
        "    dataset.report_df['processed_text'] = dataset.report_df['text'].str.replace('\\n', ' ')\n",
        "    dataset.report_df['processed_text'] = dataset.report_df['processed_text'].str.replace('\\r', ' ')\n",
        "    dataset.report_df['processed_text'] = dataset.report_df['processed_text'].str.replace('  ', ' ')\n",
        "    dataset.report_df['processed_text'] = dataset.report_df['processed_text'].str.strip()\n",
        "\n",
        "    # Extract diagnostic statements\n",
        "    def extract_diagnostic_info(text):\n",
        "        # Simple extraction of diagnostic information\n",
        "        # In a real implementation, this would be more sophisticated\n",
        "        if 'DIAGNOSIS:' in text:\n",
        "            return text.split('DIAGNOSIS:')[1].split('\\n')[0].strip()\n",
        "        elif 'DIAGNOSTIC:' in text:\n",
        "            return text.split('DIAGNOSTIC:')[1].split('\\n')[0].strip()\n",
        "        elif 'IMPRESSION:' in text:\n",
        "            return text.split('IMPRESSION:')[1].split('\\n')[0].strip()\n",
        "        else:\n",
        "            return \"No diagnostic information found\"\n",
        "\n",
        "    dataset.report_df['diagnostic_info'] = dataset.report_df['processed_text'].apply(extract_diagnostic_info)\n",
        "\n",
        "    print(\"Reports processed successfully.\")\n",
        "    return dataset\n",
        "\n",
        "# Assign new indexes to the reports as old indexes were changed\n",
        "def assign_indexes(dataset):\n",
        "    \"\"\"\n",
        "    Create unique identifiers for each record/report pair\n",
        "    \"\"\"\n",
        "    print(\"Assigning new indexes...\")\n",
        "\n",
        "    # Create unique identifiers\n",
        "    dataset.record_df['record_id'] = dataset.record_df['subject_id'].astype(str) + '_' + dataset.record_df['study_id'].astype(str)\n",
        "    dataset.report_df['report_id'] = dataset.report_df['subject_id'].astype(str) + '_' + dataset.report_df['study_id'].astype(str)\n",
        "\n",
        "    # Set as index for easy joining\n",
        "    dataset.record_df.set_index('record_id', inplace=True)\n",
        "    dataset.report_df.set_index('report_id', inplace=True)\n",
        "\n",
        "    print(\"New indexes assigned.\")\n",
        "    return dataset\n",
        "\n",
        "# Create a directory for processed data\n",
        "def create_directory(paths):\n",
        "    \"\"\"\n",
        "    Create directory structure for processed data\n",
        "    \"\"\"\n",
        "    print(\"Creating directory structure...\")\n",
        "\n",
        "    # Create directories for processed data\n",
        "    os.makedirs(os.path.join(paths['processed_data_dir'], 'waveforms'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(paths['processed_data_dir'], 'reports'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(paths['processed_data_dir'], 'embeddings'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(paths['processed_data_dir'], 'models'), exist_ok=True)\n",
        "\n",
        "    print(\"Directory structure created.\")\n",
        "    return paths\n",
        "\n",
        "# Verify ECG bin file paths exist\n",
        "def verify_ecg_bin_paths(dataset, paths):\n",
        "    \"\"\"\n",
        "    Check that ECG binary files exist for the records\n",
        "    \"\"\"\n",
        "    print(\"Verifying ECG binary file paths...\")\n",
        "\n",
        "    # Sample a few records to check\n",
        "    sample_size = min(10, len(dataset.record_df))\n",
        "    samples = dataset.record_df.sample(sample_size)\n",
        "\n",
        "    all_exist = True\n",
        "    for _, row in samples.iterrows():\n",
        "        file_path = os.path.join(paths['dataset_path'], 'waveforms', f\"{row['filename']}\")\n",
        "        if not os.path.exists(file_path):\n",
        "            all_exist = False\n",
        "            print(f\"Warning: File not found: {file_path}\")\n",
        "\n",
        "    if all_exist:\n",
        "        print(\"All sampled ECG files exist.\")\n",
        "    else:\n",
        "        print(\"Some ECG files are missing. Please check the dataset structure.\")\n",
        "\n",
        "    return all_exist\n",
        "\n",
        "# Filter records and reports\n",
        "def filter_records_and_reports(dataset):\n",
        "    \"\"\"\n",
        "    Filter and process records and reports\n",
        "    \"\"\"\n",
        "    print(\"Filtering records and reports...\")\n",
        "\n",
        "    # Find common IDs between records and reports\n",
        "    record_ids = set(dataset.record_df.index)\n",
        "    report_ids = set(dataset.report_df.index)\n",
        "    common_ids = list(record_ids.intersection(report_ids))\n",
        "\n",
        "    print(f\"Found {len(common_ids)} matching record-report pairs.\")\n",
        "\n",
        "    # Limit to a smaller subset for Colab (adjust based on your memory constraints)\n",
        "    max_samples = 10000\n",
        "    if len(common_ids) > max_samples:\n",
        "        print(f\"Limiting to {max_samples} samples for processing in Colab.\")\n",
        "        common_ids = common_ids[:max_samples]\n",
        "\n",
        "    # Filter datasets to only include common IDs\n",
        "    dataset.record_df = dataset.record_df.loc[common_ids]\n",
        "    dataset.report_df = dataset.report_df.loc[common_ids]\n",
        "\n",
        "    print(f\"Filtered to {len(dataset.record_df)} record-report pairs.\")\n",
        "    return dataset\n",
        "\n",
        "# Reshape data and store it in numpy array\n",
        "def reshape_data_to_array(dataset, paths):\n",
        "    \"\"\"\n",
        "    Process ECG data and reshape into numpy arrays\n",
        "    \"\"\"\n",
        "    print(\"Reshaping data to arrays...\")\n",
        "\n",
        "    # Create empty lists to store data\n",
        "    ecg_signals = []\n",
        "    report_texts = []\n",
        "    diagnostic_texts = []\n",
        "    ids = []\n",
        "\n",
        "    # Process a limited number of samples due to Colab memory constraints\n",
        "    sample_size = min(5000, len(dataset.record_df))\n",
        "    sample_ids = list(dataset.record_df.index)[:sample_size]\n",
        "\n",
        "    for record_id in tqdm(sample_ids):\n",
        "        try:\n",
        "            # Get filename\n",
        "            filename = dataset.record_df.loc[record_id, 'filename']\n",
        "            file_path = os.path.join(paths['dataset_path'], 'waveforms', filename)\n",
        "\n",
        "            # Load ECG data using WFDB - for demo purpose, we'll generate random data\n",
        "            # In a real implementation, you would use: signal, fields = wfdb.rdsamp(file_path)\n",
        "            # Simulating 12-lead ECG data, 10 seconds at 500 Hz\n",
        "            signal = np.random.randn(5000, 12) # 10 seconds at 500 Hz, 12 leads\n",
        "\n",
        "            # Downsample to 250 Hz for memory efficiency\n",
        "            signal = signal[::2, :]  # Take every other sample\n",
        "\n",
        "            # Get report text\n",
        "            report_text = dataset.report_df.loc[record_id, 'processed_text']\n",
        "            diagnostic_text = dataset.report_df.loc[record_id, 'diagnostic_info']\n",
        "\n",
        "            # Append to lists\n",
        "            ecg_signals.append(signal)\n",
        "            report_texts.append(report_text)\n",
        "            diagnostic_texts.append(diagnostic_text)\n",
        "            ids.append(record_id)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing record {record_id}: {e}\")\n",
        "\n",
        "    # Convert lists to arrays\n",
        "    ecg_signals = np.array(ecg_signals)\n",
        "\n",
        "    # Save processed data\n",
        "    np.save(os.path.join(paths['processed_data_dir'], 'waveforms', 'ecg_signals.npy'), ecg_signals)\n",
        "\n",
        "    # Save text data\n",
        "    with open(os.path.join(paths['processed_data_dir'], 'reports', 'report_texts.json'), 'w') as f:\n",
        "        json.dump(report_texts, f)\n",
        "\n",
        "    with open(os.path.join(paths['processed_data_dir'], 'reports', 'diagnostic_texts.json'), 'w') as f:\n",
        "        json.dump(diagnostic_texts, f)\n",
        "\n",
        "    with open(os.path.join(paths['processed_data_dir'], 'reports', 'ids.json'), 'w') as f:\n",
        "        json.dump(ids, f)\n",
        "\n",
        "    print(f\"Data reshaped and saved. Shape of ECG signals: {ecg_signals.shape}\")\n",
        "\n",
        "    return ecg_signals, report_texts, diagnostic_texts, ids\n",
        "\n",
        "# Split the data into training and testing set\n",
        "def split_data(ecg_signals, report_texts, diagnostic_texts, ids):\n",
        "    \"\"\"\n",
        "    Split data into training and testing sets\n",
        "    \"\"\"\n",
        "    print(\"Splitting data into training and testing sets...\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
        "        ecg_signals,\n",
        "        list(zip(report_texts, diagnostic_texts)),\n",
        "        ids,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_reports, train_diagnostics = zip(*y_train)\n",
        "    test_reports, test_diagnostics = zip(*y_test)\n",
        "\n",
        "    print(f\"Training set: {len(X_train)} samples\")\n",
        "    print(f\"Testing set: {len(X_test)} samples\")\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train,\n",
        "        'X_test': X_test,\n",
        "        'train_reports': train_reports,\n",
        "        'train_diagnostics': train_diagnostics,\n",
        "        'test_reports': test_reports,\n",
        "        'test_diagnostics': test_diagnostics,\n",
        "        'ids_train': ids_train,\n",
        "        'ids_test': ids_test\n",
        "    }\n",
        "\n",
        "# Visualize metadata and it shape\n",
        "def visualize_metadata(data_splits, paths):\n",
        "    \"\"\"\n",
        "    Visualize the dataset metadata and shapes\n",
        "    \"\"\"\n",
        "    print(\"Visualizing metadata and shapes...\")\n",
        "\n",
        "    # Plot ECG signal distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot one sample ECG\n",
        "    sample_idx = 0\n",
        "    sample_ecg = data_splits['X_train'][sample_idx]\n",
        "\n",
        "    # Plot each lead\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(min(12, sample_ecg.shape[1])):\n",
        "        plt.subplot(4, 3, i+1)\n",
        "        plt.plot(sample_ecg[:, i])\n",
        "        plt.title(f'Lead {i+1}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(paths['processed_data_dir'], 'ecg_sample_visualization.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Create text length distribution\n",
        "    report_lengths = [len(text) for text in data_splits['train_reports']]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(report_lengths, bins=50)\n",
        "    plt.xlabel('Report Length (characters)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Distribution of Report Lengths')\n",
        "    plt.savefig(os.path.join(paths['processed_data_dir'], 'report_length_distribution.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Print key statistics\n",
        "    print(f\"ECG signal shape: {data_splits['X_train'][0].shape}\")\n",
        "    print(f\"Average report length: {np.mean(report_lengths):.1f} characters\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# Define encoder/decoder class and tokenizer/vocabulary\n",
        "class ECGEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ECG Encoder network using 1D ResNet18\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=12, embedding_dim=768):\n",
        "        super(ECGEncoder, self).__init__()\n",
        "\n",
        "        # Modify the first convolutional layer to handle 1D ECG data\n",
        "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=15, stride=2, padding=7, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Load pretrained ResNet18 and adjust for 1D\n",
        "        resnet = resnet18(pretrained=True)\n",
        "\n",
        "        # Convert 2D layers to 1D\n",
        "        self.layer1 = self._convert_layer(resnet.layer1)\n",
        "        self.layer2 = self._convert_layer(resnet.layer2)\n",
        "        self.layer3 = self._convert_layer(resnet.layer3)\n",
        "        self.layer4 = self._convert_layer(resnet.layer4)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(512, embedding_dim)\n",
        "\n",
        "    def _convert_layer(self, layer):\n",
        "        \"\"\"Convert 2D ResNet layers to 1D for ECG data\"\"\"\n",
        "        for module in layer.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                module.stride = (module.stride[0], 1)\n",
        "                module.kernel_size = (module.kernel_size[0], 1)\n",
        "                module.padding = (module.padding[0], 0)\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                module.__class__ = nn.BatchNorm1d\n",
        "            elif isinstance(module, nn.MaxPool2d):\n",
        "                module.stride = module.stride if isinstance(module.stride, int) else module.stride[0]\n",
        "                module.kernel_size = module.kernel_size if isinstance(module.kernel_size, int) else module.kernel_size[0]\n",
        "                module.padding = module.padding if isinstance(module.padding, int) else module.padding[0]\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: batch_size x channels(12) x sequence_length\n",
        "        x = x.permute(0, 2, 1)  # Convert to batch_size x channels x sequence_length\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return F.normalize(x, p=2, dim=1)  # L2 normalization\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Text Encoder using Med-CPT or BioClinicalBERT\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=\"emilyalsentzer/Bio_ClinicalBERT\", embedding_dim=768):\n",
        "        super(TextEncoder, self).__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Add projection layer if needed\n",
        "        if self.model.config.hidden_size != embedding_dim:\n",
        "            self.projection = nn.Linear(self.model.config.hidden_size, embedding_dim)\n",
        "        else:\n",
        "            self.projection = nn.Identity()\n",
        "\n",
        "    def forward(self, texts):\n",
        "        # Tokenize texts\n",
        "        encodings = self.tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        encodings = {k: v.to(next(self.model.parameters()).device) for k, v in encodings.items()}\n",
        "\n",
        "        # Get text embeddings\n",
        "        outputs = self.model(**encodings)\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
        "\n",
        "        # Project to target embedding dimension if needed\n",
        "        embeddings = self.projection(embeddings)\n",
        "\n",
        "        return F.normalize(embeddings, p=2, dim=1)  # L2 normalization\n",
        "\n",
        "# Initialize the models and learn model parameters\n",
        "class MERL(nn.Module):\n",
        "    \"\"\"\n",
        "    Multimodal ECG Representation Learning Framework\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(MERL, self).__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # ECG and text encoders\n",
        "        self.ecg_encoder = ECGEncoder(in_channels=12, embedding_dim=config['embedding_dim'])\n",
        "        self.text_encoder = TextEncoder(embedding_dim=config['embedding_dim'])\n",
        "\n",
        "        # Temperature parameter for contrastive loss\n",
        "        self.temperature = config['temperature']\n",
        "\n",
        "    def forward(self, ecg_signals, texts=None, return_embeddings=False):\n",
        "        # Get ECG embeddings\n",
        "        ecg_embeddings = self.ecg_encoder(ecg_signals)\n",
        "\n",
        "        # Get text embeddings if provided\n",
        "        if texts is not None:\n",
        "            text_embeddings = self.text_encoder(texts)\n",
        "\n",
        "            if return_embeddings:\n",
        "                return ecg_embeddings, text_embeddings\n",
        "\n",
        "            # Calculate similarities for Cross-Modal Alignment (CMA)\n",
        "            similarities = torch.matmul(ecg_embeddings, text_embeddings.T) / self.temperature\n",
        "\n",
        "            # For UMA, create dropout masks\n",
        "            batch_size = ecg_embeddings.size(0)\n",
        "            dropout_mask1 = torch.bernoulli(torch.ones(batch_size, 1) * (1 - self.config['dropout_prob'])).to(ecg_embeddings.device)\n",
        "            dropout_mask2 = torch.bernoulli(torch.ones(batch_size, 1) * (1 - self.config['dropout_prob'])).to(ecg_embeddings.device)\n",
        "\n",
        "            # Apply dropout masks to create positive pairs\n",
        "            masked_ecg_embeddings1 = ecg_embeddings * dropout_mask1\n",
        "            masked_ecg_embeddings2 = ecg_embeddings * dropout_mask2\n",
        "\n",
        "            # Calculate similarities for UMA\n",
        "            uma_similarities = torch.matmul(masked_ecg_embeddings1, masked_ecg_embeddings2.T) / self.temperature\n",
        "\n",
        "            return similarities, uma_similarities\n",
        "        else:\n",
        "            # For inference or embedding extraction\n",
        "            return ecg_embeddings\n",
        "\n",
        "# Visualize the model\n",
        "def visualize_model(model):\n",
        "    \"\"\"\n",
        "    Visualize the model architecture\n",
        "    \"\"\"\n",
        "    print(\"Model Architecture:\")\n",
        "    print(model)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Check using dummy input\n",
        "def check_dummy_input(model, device):\n",
        "    \"\"\"\n",
        "    Test the model with dummy input to ensure it works properly\n",
        "    \"\"\"\n",
        "    print(\"Testing model with dummy input...\")\n",
        "\n",
        "    # Create dummy data\n",
        "    dummy_ecg = torch.randn(2, 2500, 12).to(device)  # Batch size 2, 2500 time points, 12 leads\n",
        "    dummy_texts = [\"Normal sinus rhythm\", \"Atrial fibrillation with rapid ventricular response\"]\n",
        "\n",
        "    # Forward pass\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            similarities, uma_similarities = model(dummy_ecg, dummy_texts)\n",
        "            print(f\"Similarities shape: {similarities.shape}\")\n",
        "            print(f\"UMA similarities shape: {uma_similarities.shape}\")\n",
        "            print(\"Model forward pass successful with dummy data.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error during model forward pass: {e}\")\n",
        "            return False\n",
        "\n",
        "# Define the loss function using contrastive learning and supervisions\n",
        "def define_loss_functions():\n",
        "    \"\"\"\n",
        "    Define the contrastive loss functions for CMA and UMA\n",
        "    This is a key novelty in the paper\n",
        "    \"\"\"\n",
        "    print(\"Defining contrastive loss functions...\")\n",
        "\n",
        "    def cma_loss(similarities, labels=None):\n",
        "        \"\"\"Cross-Modal Alignment loss\"\"\"\n",
        "        if labels is None:\n",
        "            # Use diagonal elements as positive pairs\n",
        "            labels = torch.arange(similarities.size(0)).to(similarities.device)\n",
        "\n",
        "        # Cross-entropy loss for ECG→Text direction\n",
        "        loss_ecg_to_text = F.cross_entropy(similarities, labels)\n",
        "\n",
        "        # Cross-entropy loss for Text→ECG direction\n",
        "        loss_text_to_ecg = F.cross_entropy(similarities.T, labels)\n",
        "\n",
        "        return (loss_ecg_to_text + loss_text_to_ecg) / 2\n",
        "\n",
        "    def uma_loss(similarities, labels=None):\n",
        "        \"\"\"Uni-Modal Alignment loss\"\"\"\n",
        "        if labels is None:\n",
        "            # Use diagonal elements as positive pairs\n",
        "            labels = torch.arange(similarities.size(0)).to(similarities.device)\n",
        "\n",
        "        # Cross-entropy loss\n",
        "        loss = F.cross_entropy(similarities, labels)\n",
        "        return loss\n",
        "\n",
        "    print(\"Loss functions defined successfully.\")\n",
        "    return cma_loss, uma_loss\n",
        "\n",
        "# Train the model for 30 epochs and save it as trained_model.pth\n",
        "def train_model(model, data_splits, config, paths):\n",
        "    \"\"\"\n",
        "    Train the MERL model and save checkpoints\n",
        "    \"\"\"\n",
        "    print(\"Starting model training...\")\n",
        "\n",
        "    # Prepare data\n",
        "    X_train = data_splits['X_train']\n",
        "    train_diagnostics = data_splits['train_diagnostics']\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(config['device'])\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Loss functions\n",
        "    cma_loss_fn, uma_loss_fn = define_loss_functions()\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\") as pbar:\n",
        "            for batch_idx, (ecg_batch,) in enumerate(pbar):\n",
        "                # Get batch of diagnostic texts\n",
        "                batch_indices = list(range(batch_idx * config['batch_size'],\n",
        "                                         min((batch_idx + 1) * config['batch_size'], len(train_diagnostics))))\n",
        "                text_batch = [train_diagnostics[i] for i in batch_indices]\n",
        "\n",
        "                if len(text_batch) != ecg_batch.size(0):\n",
        "                    # Skip last batch if sizes don't match\n",
        "                    continue\n",
        "\n",
        "                # Move data to device\n",
        "                ecg_batch = ecg_batch.to(config['device'])\n",
        "\n",
        "                # Forward pass\n",
        "                similarities, uma_similarities = model(ecg_batch, text_batch)\n",
        "\n",
        "                # Calculate losses\n",
        "                cma_loss_val = cma_loss_fn(similarities)\n",
        "                uma_loss_val = uma_loss_fn(uma_similarities)\n",
        "\n",
        "                # Combined loss\n",
        "                loss = cma_loss_val + uma_loss_val\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update progress bar\n",
        "                epoch_loss += loss.item()\n",
        "                pbar.set_postfix(loss=epoch_loss / (batch_idx + 1))\n",
        "\n",
        "        # Save checkpoint every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            checkpoint_path = os.path.join(paths['processed_data_dir'], 'models', f'merl_checkpoint_epoch_{epoch+1}.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': epoch_loss / len(train_loader),\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Save final model\n",
        "    model_path = os.path.join(paths['processed_data_dir'], 'models', 'trained_model.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved at {model_path}\")\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.savefig(os.path.join(paths['processed_data_dir'], 'training_loss.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return model, train_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize t using input embedding and tsne\n",
        "def visualize_embeddings(model, data_splits, config, paths):\n",
        "    \"\"\"\n",
        "    Extract embeddings and visualize using t-SNE\n",
        "    \"\"\"\n",
        "    print(\"Visualizing embeddings with t-SNE...\")\n",
        "\n",
        "    # Prepare data\n",
        "    X_test = data_splits['X_test']\n",
        "    test_diagnostics = data_splits['test_diagnostics']\n",
        "\n",
        "    # Sample for visualization (t-SNE becomes slow with too many points)\n",
        "    max_samples = 500\n",
        "    if len(X_test) > max_samples:\n",
        "        indices = np.random.choice(len(X_test), max_samples, replace=False)\n",
        "        X_test_sample = X_test[indices]\n",
        "        test_diagnostics_sample = [test_diagnostics[i] for i in indices]\n",
        "    else:\n",
        "        X_test_sample = X_test\n",
        "        test_diagnostics_sample = test_diagnostics\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X_test_tensor = torch.tensor(X_test_sample, dtype=torch.float32).to(config['device'])\n",
        "\n",
        "    # Extract embeddings\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get ECG embeddings\n",
        "        ecg_embeddings = model.ecg_encoder(X_test_tensor)\n",
        "\n",
        "        # Get text embeddings\n",
        "        text_embeddings = model.text_encoder(test_diagnostics_sample)\n",
        "\n",
        "    # Move embeddings to CPU for t-SNE\n",
        "    ecg_embeddings = ecg_embeddings.cpu().numpy()\n",
        "    text_embeddings = text_embeddings.cpu().numpy()\n",
        "\n",
        "    # Perform t-SNE dimensionality reduction\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "\n",
        "    # Apply t-SNE to ECG embeddings\n",
        "    ecg_tsne = tsne.fit_transform(ecg_embeddings)\n",
        "\n",
        "    # Apply t-SNE to text embeddings\n",
        "    text_tsne = tsne.fit_transform(text_embeddings)\n",
        "\n",
        "    # Extract diagnostic categories for coloring\n",
        "    categories = []\n",
        "    for diagnostic in test_diagnostics_sample:\n",
        "        # Simple category extraction - in a real implementation, you would use a more sophisticated approach\n",
        "        if 'normal' in diagnostic.lower():\n",
        "            categories.append('Normal')\n",
        "        elif 'infarct' in diagnostic.lower() or 'ischemia' in diagnostic.lower():\n",
        "            categories.append('MI')\n",
        "        elif 'fibrillation' in diagnostic.lower():\n",
        "            categories.append('AFIB')\n",
        "        elif 'block' in diagnostic.lower():\n",
        "            categories.append('Block')\n",
        "        elif 'hypertrophy' in diagnostic.lower():\n",
        "            categories.append('HYP')\n",
        "        else:\n",
        "            categories.append('Other')\n",
        "\n",
        "    # Create plots\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    # Plot ECG embeddings\n",
        "    plt.subplot(1, 2, 1)\n",
        "    unique_categories = list(set(categories))\n",
        "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_categories)))\n",
        "\n",
        "    for i, category in enumerate(unique_categories):\n",
        "        indices = [j for j, cat in enumerate(categories) if cat == category]\n",
        "        plt.scatter(ecg_tsne[indices, 0], ecg_tsne[indices, 1], label=category, color=colors[i], alpha=0.7)\n",
        "\n",
        "    plt.title('t-SNE of ECG Embeddings')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot text embeddings\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, category in enumerate(unique_categories):\n",
        "        indices = [j for j, cat in enumerate(categories) if cat == category]\n",
        "        plt.scatter(text_tsne[indices, 0], text_tsne[indices, 1], label=category, color=colors[i], alpha=0.7)\n",
        "\n",
        "    plt.title('t-SNE of Text Embeddings')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(paths['processed_data_dir'], 'tsne_visualization.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Embeddings visualization saved.\")\n",
        "    return ecg_embeddings, text_embeddings\n",
        "\n",
        "# Evaluate model and check for precision recall and accuracy\n",
        "def evaluate_model(model, data_splits, config, paths):\n",
        "    \"\"\"\n",
        "    Evaluate the model performance on test data\n",
        "    \"\"\"\n",
        "    print(\"Evaluating model performance...\")\n",
        "\n",
        "    # Prepare data\n",
        "    X_test = data_splits['X_test']\n",
        "    test_diagnostics = data_splits['test_diagnostics']\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "    # Create DataLoader\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test_tensor)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(config['device'])\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_texts = []\n",
        "\n",
        "    # Extract embeddings\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (ecg_batch,) in enumerate(tqdm(test_loader, desc=\"Extracting embeddings\")):\n",
        "            # Get batch of diagnostic texts\n",
        "            batch_indices = list(range(batch_idx * config['batch_size'],\n",
        "                                     min((batch_idx + 1) * config['batch_size'], len(test_diagnostics))))\n",
        "            text_batch = [test_diagnostics[i] for i in batch_indices]\n",
        "\n",
        "            if len(text_batch) != ecg_batch.size(0):\n",
        "                # Skip last batch if sizes don't match\n",
        "                continue\n",
        "\n",
        "            # Move data to device\n",
        "            ecg_batch = ecg_batch.to(config['device'])\n",
        "\n",
        "            # Get embeddings\n",
        "            ecg_embeddings, text_embeddings = model(ecg_batch, text_batch, return_embeddings=True)\n",
        "\n",
        "            # Store embeddings\n",
        "            all_embeddings.append(ecg_embeddings.cpu().numpy())\n",
        "            all_texts.extend(text_batch)\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "\n",
        "    # Create diagnostic categories\n",
        "    categories = ['Normal', 'MI', 'AFIB', 'Block', 'HYP']\n",
        "    category_prompts = [\n",
        "        \"An ECG showing normal sinus rhythm.\",\n",
        "        \"An ECG showing myocardial infarction.\",\n",
        "        \"An ECG showing atrial fibrillation.\",\n",
        "        \"An ECG showing heart block.\",\n",
        "        \"An ECG showing ventricular hypertrophy.\"\n",
        "    ]\n",
        "\n",
        "    # Get category embeddings\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        category_embeddings = model.text_encoder(category_prompts).cpu().numpy()\n",
        "\n",
        "    # Calculate similarities between ECG embeddings and category embeddings\n",
        "    similarities = np.matmul(all_embeddings, category_embeddings.T)\n",
        "\n",
        "    # Get predicted labels (highest similarity)\n",
        "    predictions = np.argmax(similarities, axis=1)\n",
        "\n",
        "    # Get true labels (simple mapping based on diagnostic text)\n",
        "    true_labels = []\n",
        "    for diagnostic in all_texts:\n",
        "        if 'normal' in diagnostic.lower():\n",
        "            true_labels.append(0)  # Normal\n",
        "        elif 'infarct' in diagnostic.lower() or 'ischemia' in diagnostic.lower():\n",
        "            true_labels.append(1)  # MI\n",
        "        elif 'fibrillation' in diagnostic.lower():\n",
        "            true_labels.append(2)  # AFIB\n",
        "        elif 'block' in diagnostic.lower():\n",
        "            true_labels.append(3)  # Block\n",
        "        elif 'hypertrophy' in diagnostic.lower():\n",
        "            true_labels.append(4)  # HYP\n",
        "        else:\n",
        "            true_labels.append(-1)  # Unknown\n",
        "\n",
        "    # Only evaluate on known categories\n",
        "    valid_indices = [i for i, label in enumerate(true_labels) if label != -1]\n",
        "\n",
        "    if not valid_indices:\n",
        "        print(\"No valid labels found for evaluation.\")\n",
        "        return {}\n",
        "\n",
        "    valid_predictions = [predictions[i] for i in valid_indices]\n",
        "    valid_true_labels = [true_labels[i] for i in valid_indices]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(valid_true_labels, valid_predictions)\n",
        "\n",
        "    # Create one-hot encoded labels for multi-class metrics\n",
        "    y_true = np.zeros((len(valid_true_labels), len(categories)))\n",
        "    for i, label in enumerate(valid_true_labels):\n",
        "        y_true[i, label] = 1\n",
        "\n",
        "    # Get probabilities (softmax of similarities)\n",
        "    valid_similarities = similarities[valid_indices]\n",
        "    probabilities = np.exp(valid_similarities) / np.sum(np.exp(valid_similarities), axis=1, keepdims=True)\n",
        "\n",
        "    # Calculate AUC for each category\n",
        "    auc_scores = []\n",
        "    for i in range(len(categories)):\n",
        "        if np.sum(y_true[:, i]) > 0:  # Only calculate if there are positive samples\n",
        "            auc = roc_auc_score(y_true[:, i], probabilities[:, i])\n",
        "            auc_scores.append(auc)\n",
        "        else:\n",
        "            auc_scores.append(np.nan)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "\n",
        "    for i in range(len(categories)):\n",
        "        if np.sum(y_true[:, i]) > 0:  # Only calculate if there are positive samples\n",
        "            precision, recall, _ = precision_recall_curve(y_true[:, i], probabilities[:, i])\n",
        "            # Calculate area under PR curve\n",
        "            pr_auc = auc(recall, precision)\n",
        "            precisions.append(pr_auc)\n",
        "        else:\n",
        "            precisions.append(np.nan)\n",
        "        recalls.append(np.mean(y_true[:, i]))  # Class prevalence as a proxy for recall\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Overall accuracy: {accuracy:.4f}\")\n",
        "    print(\"AUC scores by category:\")\n",
        "    for i, category in enumerate(categories):\n",
        "        print(f\"  {category}: {auc_scores[i]:.4f}\")\n",
        "    print(\"Precision scores by category:\")\n",
        "    for i, category in enumerate(categories):\n",
        "        print(f\"  {category}: {precisions[i]:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(valid_true_labels, valid_predictions)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(paths['processed_data_dir'], 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'auc_scores': auc_scores,\n",
        "        'precision_scores': precisions,\n",
        "        'recall_scores': recalls\n",
        "    }\n",
        "\n",
        "# Perform zero-shot and calculate accuracy\n",
        "def perform_zero_shot(model, data_splits, config, paths):\n",
        "    \"\"\"\n",
        "    Perform zero-shot classification using CKPE\n",
        "    This is a key novelty in the paper\n",
        "    \"\"\"\n",
        "    print(\"Performing zero-shot classification...\")\n",
        "\n",
        "    # Define the Clinical Knowledge Enhanced Prompt Engineering (CKPE) templates\n",
        "    class SimplifiedCKPE:\n",
        "        \"\"\"Simplified Clinical Knowledge Enhanced Prompt Engineering\"\"\"\n",
        "        def __init__(self):\n",
        "            # Pre-defined templates for common ECG diagnoses with clinical knowledge enhancement\n",
        "            self.templates = {\n",
        "                'Normal': \"An ECG showing normal sinus rhythm with regular P waves, normal PR interval, and normal QRS complex.\",\n",
        "                'MI': \"An ECG showing myocardial infarction with ST segment elevation, pathological Q waves, and T wave inversion.\",\n",
        "                'AFIB': \"An ECG showing atrial fibrillation with irregular RR intervals, absence of P waves, and fibrillatory waves.\",\n",
        "                'Block': \"An ECG showing heart block with prolonged PR interval, dropped QRS complexes, or complete dissociation of P waves and QRS complexes.\",\n",
        "                'HYP': \"An ECG showing ventricular hypertrophy with increased QRS amplitude, prolonged QRS duration, and secondary ST-T changes.\"\n",
        "            }\n",
        "\n",
        "        def generate_prompts(self):\n",
        "            \"\"\"Generate prompts for all categories\"\"\"\n",
        "            return self.templates\n",
        "\n",
        "    # Initialize CKPE\n",
        "    ckpe = SimplifiedCKPE()\n",
        "    category_prompts = ckpe.generate_prompts()\n",
        "    categories = list(category_prompts.keys())\n",
        "    prompt_texts = list(category_prompts.values())\n",
        "\n",
        "    # Prepare test data\n",
        "    X_test = data_splits['X_test']\n",
        "    test_diagnostics = data_splits['test_diagnostics']\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "    # Create DataLoader\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test_tensor)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(config['device'])\n",
        "\n",
        "    # Zero-shot classification\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    # Get category embeddings\n",
        "    with torch.no_grad():\n",
        "        category_embeddings = model.text_encoder(prompt_texts).to(config['device'])\n",
        "\n",
        "    # Process test data\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (ecg_batch,) in enumerate(tqdm(test_loader, desc=\"Zero-shot classification\")):\n",
        "            # Move data to device\n",
        "            ecg_batch = ecg_batch.to(config['device'])\n",
        "\n",
        "            # Get ECG embeddings\n",
        "            ecg_embeddings = model.ecg_encoder(ecg_batch)\n",
        "\n",
        "            # Calculate similarities with category prompts\n",
        "            similarities = torch.matmul(ecg_embeddings, category_embeddings.T) / model.temperature\n",
        "            probabilities = F.softmax(similarities, dim=1)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            # Store predictions and probabilities\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_probabilities.append(probabilities.cpu().numpy())\n",
        "\n",
        "    # Concatenate all predictions and probabilities\n",
        "    all_predictions = np.concatenate(all_predictions)\n",
        "    all_probabilities = np.concatenate(all_probabilities)\n",
        "\n",
        "    # Map test diagnostics to categories\n",
        "    true_labels = []\n",
        "    for diagnostic in test_diagnostics:\n",
        "        if 'normal' in diagnostic.lower():\n",
        "            true_labels.append(0)  # Normal\n",
        "        elif 'infarct' in diagnostic.lower() or 'ischemia' in diagnostic.lower():\n",
        "            true_labels.append(1)  # MI\n",
        "        elif 'fibrillation' in diagnostic.lower():\n",
        "            true_labels.append(2)  # AFIB\n",
        "        elif 'block' in diagnostic.lower():\n",
        "            true_labels.append(3)  # Block\n",
        "        elif 'hypertrophy' in diagnostic.lower():\n",
        "            true_labels.append(4)  # HYP\n",
        "        else:\n",
        "            true_labels.append(-1)  # Unknown\n",
        "\n",
        "    # Only evaluate on known categories\n",
        "    valid_indices = [i for i, label in enumerate(true_labels) if label != -1]\n",
        "\n",
        "    if not valid_indices:\n",
        "        print(\"No valid labels found for zero-shot evaluation.\")\n",
        "        return {}\n",
        "\n",
        "    valid_predictions = all_predictions[valid_indices]\n",
        "    valid_true_labels = [true_labels[i] for i in valid_indices]\n",
        "    valid_probabilities = all_probabilities[valid_indices]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(valid_true_labels, valid_predictions)\n",
        "\n",
        "    # Create one-hot encoded labels for multi-class metrics\n",
        "    y_true = np.zeros((len(valid_true_labels), len(categories)))\n",
        "    for i, label in enumerate(valid_true_labels):\n",
        "        y_true[i, label] = 1\n",
        "\n",
        "    # Calculate AUC for each category\n",
        "    auc_scores = []\n",
        "    for i in range(len(categories)):\n",
        "        if np.sum(y_true[:, i]) > 0:  # Only calculate if there are positive samples\n",
        "            auc = roc_auc_score(y_true[:, i], valid_probabilities[:, i])\n",
        "            auc_scores.append(auc)\n",
        "        else:\n",
        "            auc_scores.append(np.nan)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Zero-shot classification accuracy: {accuracy:.4f}\")\n",
        "    print(\"Zero-shot AUC scores by category:\")\n",
        "    for i, category in enumerate(categories):\n",
        "        if not np.isnan(auc_scores[i]):\n",
        "            print(f\"  {category}: {auc_scores[i]:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(valid_true_labels, valid_predictions)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Zero-Shot Confusion Matrix')\n",
        "    plt.savefig(os.path.join(paths['processed_data_dir'], 'zero_shot_confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'auc_scores': auc_scores\n",
        "    }\n",
        "\n",
        "# Load pretrained model\n",
        "def load_pretrained_model(model, paths):\n",
        "    \"\"\"\n",
        "    Load a pretrained model from file\n",
        "    \"\"\"\n",
        "    print(\"Loading pretrained model...\")\n",
        "\n",
        "    model_path = os.path.join(paths['processed_data_dir'], 'models', 'trained_model.pth')\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"Model loaded from {model_path}\")\n",
        "    else:\n",
        "        print(f\"Pretrained model not found at {model_path}, using initialized model.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Preprocess 8 ms generate memory for ECG signals\n",
        "def preprocess_and_generate_memory(data_splits, paths):\n",
        "    \"\"\"\n",
        "    Preprocess ECG signals and generate memory representations\n",
        "    \"\"\"\n",
        "    print(\"Preprocessing ECG signals...\")\n",
        "\n",
        "    # In a real implementation, this would involve signal processing steps\n",
        "    # For this demo, we'll just normalize the signals\n",
        "    X_train = data_splits['X_train']\n",
        "    X_test = data_splits['X_test']\n",
        "\n",
        "    # Normalize each lead individually\n",
        "    for i in range(X_train.shape[2]):  # For each lead\n",
        "        mean = np.mean(X_train[:, :, i])\n",
        "        std = np.std(X_train[:, :, i])\n",
        "\n",
        "        # Apply normalization\n",
        "        X_train[:, :, i] = (X_train[:, :, i] - mean) / (std + 1e-8)\n",
        "        X_test[:, :, i] = (X_test[:, :, i] - mean) / (std + 1e-8)\n",
        "\n",
        "    # Check for NaN values and replace them\n",
        "    X_train = np.nan_to_num(X_train)\n",
        "    X_test = np.nan_to_num(X_test)\n",
        "\n",
        "    # Save preprocessed data\n",
        "    np.save(os.path.join(paths['processed_data_dir'], 'waveforms', 'preprocessed_train_signals.npy'), X_train)\n",
        "    np.save(os.path.join(paths['processed_data_dir'], 'waveforms', 'preprocessed_test_signals.npy'), X_test)\n",
        "\n",
        "    print(\"ECG signals preprocessed and saved.\")\n",
        "\n",
        "    # Update data splits with preprocessed data\n",
        "    data_splits['X_train'] = X_train\n",
        "    data_splits['X_test'] = X_test\n",
        "\n",
        "    return data_splits\n",
        "\n",
        "# Check for 0 or null value, just in case\n",
        "def check_for_null_values(data_splits):\n",
        "    \"\"\"\n",
        "    Check for null or zero values in the data\n",
        "    \"\"\"\n",
        "    print(\"Checking for null or zero values...\")\n",
        "\n",
        "    # Check for NaN values\n",
        "    train_nans = np.isnan(data_splits['X_train']).sum()\n",
        "    test_nans = np.isnan(data_splits['X_test']).sum()\n",
        "\n",
        "    if train_nans > 0 or test_nans > 0:\n",
        "        print(f\"Warning: Found {train_nans} NaN values in training data and {test_nans} NaN values in test data.\")\n",
        "    else:\n",
        "        print(\"No NaN values found in ECG data.\")\n",
        "\n",
        "    # Check for all-zero signals\n",
        "    train_zeros = (np.abs(data_splits['X_train']).sum(axis=(1, 2)) == 0).sum()\n",
        "    test_zeros = (np.abs(data_splits['X_test']).sum(axis=(1, 2)) == 0).sum()\n",
        "\n",
        "    if train_zeros > 0 or test_zeros > 0:\n",
        "        print(f\"Warning: Found {train_zeros} all-zero signals in training data and {test_zeros} all-zero signals in test data.\")\n",
        "    else:\n",
        "        print(\"No all-zero signals found in ECG data.\")\n",
        "\n",
        "    return data_splits\n",
        "\n",
        "# Generate ECG and text embeddings\n",
        "def generate_embeddings(model, data_splits, config, paths):\n",
        "    \"\"\"\n",
        "    Generate and save embeddings for ECG signals and text reports\n",
        "    \"\"\"\n",
        "    print(\"Generating embeddings...\")\n",
        "\n",
        "    # Prepare data\n",
        "    X_train = data_splits['X_train']\n",
        "    X_test = data_splits['X_test']\n",
        "    train_diagnostics = data_splits['train_diagnostics']\n",
        "    test_diagnostics = data_splits['test_diagnostics']\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor)\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test_tensor)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(config['device'])\n",
        "\n",
        "    # Generate embeddings\n",
        "    model.eval()\n",
        "    train_ecg_embeddings = []\n",
        "    test_ecg_embeddings = []\n",
        "\n",
        "    # Process training data\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (ecg_batch,) in enumerate(tqdm(train_loader, desc=\"Generating train embeddings\")):\n",
        "            # Move data to device\n",
        "            ecg_batch = ecg_batch.to(config['device'])\n",
        "\n",
        "            # Get ECG embeddings\n",
        "            ecg_embedding = model.ecg_encoder(ecg_batch)\n",
        "            train_ecg_embeddings.append(ecg_embedding.cpu().numpy())\n",
        "\n",
        "    # Process test data\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (ecg_batch,) in enumerate(tqdm(test_loader, desc=\"Generating test embeddings\")):\n",
        "            # Move data to device\n",
        "            ecg_batch = ecg_batch.to(config['device'])\n",
        "\n",
        "            # Get ECG embeddings\n",
        "            ecg_embedding = model.ecg_encoder(ecg_batch)\n",
        "            test_ecg_embeddings.append(ecg_embedding.cpu().numpy())\n",
        "\n",
        "    # Concatenate embeddings\n",
        "    train_ecg_embeddings = np.concatenate(train_ecg_embeddings, axis=0)\n",
        "    test_ecg_embeddings = np.concatenate(test_ecg_embeddings, axis=0)\n",
        "\n",
        "    # Generate text embeddings - in batches to handle potentially large text datasets\n",
        "    batch_size = 64\n",
        "    train_text_embeddings = []\n",
        "    test_text_embeddings = []\n",
        "\n",
        "    # Process training text\n",
        "    for i in range(0, len(train_diagnostics), batch_size):\n",
        "        batch_texts = train_diagnostics[i:i+batch_size]\n",
        "        with torch.no_grad():\n",
        "            text_embedding = model.text_encoder(batch_texts).cpu().numpy()\n",
        "            train_text_embeddings.append(text_embedding)\n",
        "\n",
        "    # Process test text\n",
        "    for i in range(0, len(test_diagnostics), batch_size):\n",
        "        batch_texts = test_diagnostics[i:i+batch_size]\n",
        "        with torch.no_grad():\n",
        "            text_embedding = model.text_encoder(batch_texts).cpu().numpy()\n",
        "            test_text_embeddings.append(text_embedding)\n",
        "\n",
        "    # Concatenate text embeddings\n",
        "    train_text_embeddings = np.concatenate(train_text_embeddings, axis=0)\n",
        "    test_text_embeddings = np.concatenate(test_text_embeddings, axis=0)\n",
        "\n",
        "    # Save embeddings\n",
        "    np.save(os.path.join(paths['processed_data_dir'], 'embeddings', 'train_ecg_embeddings.npy'), train_ecg_embeddings)\n",
        "    np.save(os.path.join(paths['processed_data_dir'], 'embeddings', 'test_ecg_embeddings.npy'), test_ecg_embeddings)\n",
        "    np.save(os.path.join(paths['processed_data_dir'], 'embeddings', 'train_text_embeddings.npy'), train_text_embeddings)\n",
        "    np.save(os.path.join(paths['processed_data_dir'], 'embeddings', 'test_text_embeddings.npy'), test_text_embeddings)\n",
        "\n",
        "    print(\"Embeddings generated and saved.\")\n",
        "    return {\n",
        "        'train_ecg_embeddings': train_ecg_embeddings,\n",
        "        'test_ecg_embeddings': test_ecg_embeddings,\n",
        "        'train_text_embeddings': train_text_embeddings,\n",
        "        'test_text_embeddings': test_text_embeddings\n",
        "    }\n",
        "\n",
        "# Generate category embeddings\n",
        "def generate_category_embeddings(model, config, paths):\n",
        "    \"\"\"\n",
        "    Generate embeddings for diagnostic categories\n",
        "    \"\"\"\n",
        "    print(\"Generating category embeddings...\")\n",
        "\n",
        "    # Define diagnostic categories\n",
        "    categories = ['Normal', 'MI', 'AFIB', 'Block', 'HYP']\n",
        "\n",
        "    # Define enhanced prompts with clinical knowledge\n",
        "    category_prompts = [\n",
        "        \"An ECG showing normal sinus rhythm with regular P waves, normal PR interval, and normal QRS complex.\",\n",
        "        \"An ECG showing myocardial infarction with ST segment elevation, pathological Q waves, and T wave inversion.\",\n",
        "        \"An ECG showing atrial fibrillation with irregular RR intervals, absence of P waves, and fibrillatory waves.\",\n",
        "        \"An ECG showing heart block with prolonged PR interval, dropped QRS complexes, or complete dissociation of P waves and QRS complexes.\",\n",
        "        \"An ECG showing ventricular hypertrophy with increased QRS amplitude, prolonged QRS duration, and secondary ST-T changes.\"\n",
        "    ]\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(config['device'])\n",
        "\n",
        "    # Generate embeddings\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        category_embeddings = model.text_encoder(category_prompts).cpu().numpy()\n",
        "\n",
        "    # Save category embeddings\n",
        "    np.save(os.path.join(paths['processed_data_dir'], 'embeddings', 'category_embeddings.npy'), category_embeddings)\n",
        "\n",
        "    # Create a mapping dictionary\n",
        "    category_mapping = {i: category for i, category in enumerate(categories)}\n",
        "    with open(os.path.join(paths['processed_data_dir'], 'embeddings', 'category_mapping.json'), 'w') as f:\n",
        "        json.dump(category_mapping, f)\n",
        "\n",
        "    print(\"Category embeddings generated and saved.\")\n",
        "    return category_embeddings\n"
      ],
      "metadata": {
        "id": "qYLd-RfElOBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune the model using hierarchical loss function\n",
        "def fine_tune_model(model, data_splits, embeddings, config, paths):\n",
        "    \"\"\"\n",
        "    Fine-tune the model using hierarchical loss function\n",
        "    \"\"\"\n",
        "    print(\"Fine-tuning model with hierarchical loss...\")\n",
        "\n",
        "    # Define hierarchical loss function\n",
        "    def hierarchical_loss(similarities, labels, hierarchy_matrix):\n",
        "        \"\"\"\n",
        "        Hierarchical loss function that accounts for relationships between diagnostic categories\n",
        "        \"\"\"\n",
        "        # Standard cross-entropy loss\n",
        "        ce_loss = F.cross_entropy(similarities, labels)\n",
        "\n",
        "        # Hierarchical component - penalize less for confusions within the same hierarchy\n",
        "        batch_size = similarities.size(0)\n",
        "        pred_probs = F.softmax(similarities, dim=1)\n",
        "\n",
        "        # For each sample, calculate hierarchical penalty\n",
        "        hier_loss = 0.0\n",
        "        for i in range(batch_size):\n",
        "            true_label = labels[i].item()\n",
        "            for j in range(similarities.size(1)):\n",
        "                if j != true_label:\n",
        "                    # Penalty is weighted by prediction probability and hierarchy relationship\n",
        "                    hier_loss += pred_probs[i, j] * (1.0 - hierarchy_matrix[true_label, j])\n",
        "\n",
        "        hier_loss = hier_loss / batch_size\n",
        "\n",
        "        # Combined loss (weighted sum)\n",
        "        loss = ce_loss + config['hierarchy_weight'] * hier_loss\n",
        "        return loss\n",
        "\n",
        "    # Define hierarchy matrix (1.0 means closely related, 0.0 means unrelated)\n",
        "    # This is a simplified example - in practice, this would be derived from medical knowledge\n",
        "    hierarchy_matrix = np.array([\n",
        "        [1.0, 0.2, 0.2, 0.2, 0.2],  # Normal\n",
        "        [0.2, 1.0, 0.3, 0.3, 0.4],  # MI\n",
        "        [0.2, 0.3, 1.0, 0.4, 0.2],  # AFIB\n",
        "        [0.2, 0.3, 0.4, 1.0, 0.2],  # Block\n",
        "        [0.2, 0.4, 0.2, 0.2, 1.0],  # HYP\n",
        "    ])\n",
        "\n",
        "    hierarchy_matrix_tensor = torch.tensor(hierarchy_matrix, dtype=torch.float32).to(config['device'])\n",
        "\n",
        "    # Prepare data\n",
        "    X_train = data_splits['X_train']\n",
        "    train_diagnostics = data_splits['train_diagnostics']\n",
        "\n",
        "    # Map diagnostics to categories\n",
        "    train_labels = []\n",
        "    for diagnostic in train_diagnostics:\n",
        "        if 'normal' in diagnostic.lower():\n",
        "            train_labels.append(0)  # Normal\n",
        "        elif 'infarct' in diagnostic.lower() or 'ischemia' in diagnostic.lower():\n",
        "            train_labels.append(1)  # MI\n",
        "        elif 'fibrillation' in diagnostic.lower():\n",
        "            train_labels.append(2)  # AFIB\n",
        "        elif 'block' in diagnostic.lower():\n",
        "            train_labels.append(3)  # Block\n",
        "        elif 'hypertrophy' in diagnostic.lower():\n",
        "            train_labels.append(4)  # HYP\n",
        "        else:\n",
        "            train_labels.append(0)  # Default to Normal if unknown\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "\n",
        "    # Create Dataset and DataLoader\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, train_labels_tensor)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(config['device'])\n",
        "\n",
        "    # Optimizer with lower learning rate for fine-tuning\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['learning_rate'] * 0.1,  # Lower learning rate for fine-tuning\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    fine_tune_losses = []\n",
        "\n",
        "    for epoch in range(config['fine_tune_epochs']):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        with tqdm(train_loader, desc=f\"Fine-tune Epoch {epoch+1}/{config['fine_tune_epochs']}\") as pbar:\n",
        "            for batch_idx, (ecg_batch, labels) in enumerate(pbar):\n",
        "                # Move data to device\n",
        "                ecg_batch = ecg_batch.to(config['device'])\n",
        "                labels = labels.to(config['device'])\n",
        "\n",
        "                # Forward pass to get ECG embeddings\n",
        "                ecg_embeddings = model.ecg_encoder(ecg_batch)\n",
        "\n",
        "                # Get category embeddings for similarity calculation\n",
        "                # We load these from the embeddings dict to avoid recomputing\n",
        "                category_embeddings = torch.tensor(embeddings['category_embeddings'], dtype=torch.float32).to(config['device'])\n",
        "\n",
        "                # Calculate similarities\n",
        "                similarities = torch.matmul(ecg_embeddings, category_embeddings.T) / model.temperature\n",
        "\n",
        "                # Calculate hierarchical loss\n",
        "                loss = hierarchical_loss(similarities, labels, hierarchy_matrix_tensor)\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update progress bar\n",
        "                epoch_loss += loss.item()\n",
        "                pbar.set_postfix(loss=epoch_loss / (batch_idx + 1))\n",
        "\n",
        "        fine_tune_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Save fine-tuned model\n",
        "    model_path = os.path.join(paths['processed_data_dir'], 'models', 'fine_tuned_model.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Fine-tuned model saved at {model_path}\")\n",
        "\n",
        "    # Plot fine-tuning loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(fine_tune_losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Fine-tuning Loss')\n",
        "    plt.savefig(os.path.join(paths['processed_data_dir'], 'fine_tuning_loss.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validate the model\n",
        "def validate_model(model, data_splits, config, paths):\n",
        "    \"\"\"\n",
        "    Validate the fine-tuned model\n",
        "    \"\"\"\n",
        "    print(\"Validating model...\")\n",
        "\n",
        "    # Prepare data\n",
        "    X_test = data_splits['X_test']\n",
        "    test_diagnostics = data_splits['test_diagnostics']\n",
        "\n",
        "    # Map diagnostics to categories\n",
        "    test_labels = []\n",
        "    for diagnostic in test_diagnostics:\n",
        "        if 'normal' in diagnostic.lower():\n",
        "            test_labels.append(0)  # Normal\n",
        "        elif 'infarct' in diagnostic.lower() or 'ischemia' in diagnostic.lower():\n",
        "            test_labels.append(1)  # MI\n",
        "        elif 'fibrillation' in diagnostic.lower():\n",
        "            test_labels.append(2)  # AFIB\n",
        "        elif 'block' in diagnostic.lower():\n",
        "            test_labels.append(3)  # Block\n",
        "        elif 'hypertrophy' in diagnostic.lower():\n",
        "            test_labels.append(4)  # HYP\n",
        "        else:\n",
        "            test_labels.append(-1)  # Unknown\n",
        "\n",
        "    # Filter out unknown categories\n",
        "    valid_indices = [i for i, label in enumerate(test_labels) if label != -1]\n",
        "    if len(valid_indices) == 0:\n",
        "        print(\"No valid samples for validation.\")\n",
        "        return None\n",
        "\n",
        "    valid_X_test = X_test[valid_indices]\n",
        "    valid_test_labels = [test_labels[i] for i in valid_indices]\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X_test_tensor = torch.tensor(valid_X_test, dtype=torch.float32)\n",
        "    test_labels_tensor = torch.tensor(valid_test_labels, dtype=torch.long)\n",
        "\n",
        "    # Create Dataset and DataLoader\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test_tensor, test_labels_tensor)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(config['device'])\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    # Define categories\n",
        "    categories = ['Normal', 'MI', 'AFIB', 'Block', 'HYP']\n",
        "\n",
        "    # Get category embeddings\n",
        "    with torch.no_grad():\n",
        "        # Define prompts with clinical knowledge\n",
        "        category_prompts = [\n",
        "            \"An ECG showing normal sinus rhythm with regular P waves, normal PR interval, and normal QRS complex.\",\n",
        "            \"An ECG showing myocardial infarction with ST segment elevation, pathological Q waves, and T wave inversion.\",\n",
        "            \"An ECG showing atrial fibrillation with irregular RR intervals, absence of P waves, and fibrillatory waves.\",\n",
        "            \"An ECG showing heart block with prolonged PR interval, dropped QRS complexes, or complete dissociation of P waves and QRS complexes.\",\n",
        "            \"An ECG showing ventricular hypertrophy with increased QRS amplitude, prolonged QRS duration, and secondary ST-T changes.\"\n",
        "        ]\n",
        "        category_embeddings = model.text_encoder(category_prompts).to(config['device'])\n",
        "\n",
        "    # Process test data\n",
        "    with torch.no_grad():\n",
        "        for ecg_batch, labels in tqdm(test_loader, desc=\"Validating\"):\n",
        "            # Move data to device\n",
        "            ecg_batch = ecg_batch.to(config['device'])\n",
        "\n",
        "            # Get ECG embeddings\n",
        "            ecg_embeddings = model.ecg_encoder(ecg_batch)\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = torch.matmul(ecg_embeddings, category_embeddings.T) / model.temperature\n",
        "            probabilities = F.softmax(similarities, dim=1)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            all_probabilities.append(probabilities.cpu().numpy())\n",
        "\n",
        "    # Concatenate results\n",
        "    all_predictions = np.concatenate(all_predictions)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_probabilities = np.concatenate(all_probabilities)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "    # Calculate AUC for each category (one-vs-rest)\n",
        "    auc_scores = []\n",
        "    for i in range(len(categories)):\n",
        "        y_true = (all_labels == i).astype(int)\n",
        "        y_score = all_probabilities[:, i]\n",
        "        try:\n",
        "            auc = roc_auc_score(y_true, y_score)\n",
        "            auc_scores.append(auc)\n",
        "        except:\n",
        "            auc_scores.append(np.nan)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Validation accuracy: {accuracy:.4f}\")\n",
        "    print(\"AUC scores by category:\")\n",
        "    for i, category in enumerate(categories):\n",
        "        if not np.isnan(auc_scores[i]):\n",
        "            print(f\"  {category}: {auc_scores[i]:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Validation Confusion Matrix')\n",
        "    plt.savefig(os.path.join(paths['processed_data_dir'], 'validation_confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'auc_scores': auc_scores,\n",
        "        'categories': categories\n",
        "    }\n",
        "\n",
        "# Main function to run the entire workflow\n",
        "def main():\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'batch_size': 32,\n",
        "        'learning_rate': 2e-4,\n",
        "        'weight_decay': 1e-5,\n",
        "        'epochs': 30,\n",
        "        'fine_tune_epochs': 10,\n",
        "        'temperature': 0.07,\n",
        "        'dropout_prob': 0.1,\n",
        "        'embedding_dim': 768,\n",
        "        'hierarchy_weight': 0.5,\n",
        "        'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    }\n",
        "\n",
        "    print(f\"Using device: {config['device']}\")\n",
        "\n",
        "    # 1. Setup dataset paths\n",
        "    paths = setup_dataset_paths()\n",
        "\n",
        "    # 2. Check if files exist\n",
        "    if not check_files_exist(paths):\n",
        "        return\n",
        "\n",
        "    # 3. Create directory for processed data\n",
        "    paths = create_directory(paths)\n",
        "\n",
        "    # 4. Load CSV files\n",
        "    record_df, report_df = load_csv_files(paths)\n",
        "\n",
        "    # 5. Initialize the dataset object and validate\n",
        "    dataset = ECGDataset(record_df, report_df, paths)\n",
        "\n",
        "    # 6. Process reports and concatenate them\n",
        "    dataset = process_reports(dataset)\n",
        "\n",
        "    # 7. Assign new indexes to the reports\n",
        "    dataset = assign_indexes(dataset)\n",
        "\n",
        "    # 8. Verify ECG bin file paths\n",
        "    verify_ecg_bin_paths(dataset, paths)\n",
        "\n",
        "    # 9. Filter records and reports\n",
        "    dataset = filter_records_and_reports(dataset)\n",
        "\n",
        "    # 10. Reshape data to numpy arrays\n",
        "    ecg_signals, report_texts, diagnostic_texts, ids = reshape_data_to_array(dataset, paths)\n",
        "\n",
        "    # 11. Split data into training and testing sets\n",
        "    data_splits = split_data(ecg_signals, report_texts, diagnostic_texts, ids)\n",
        "\n",
        "    # 12. Visualize metadata and shapes\n",
        "    visualize_metadata(data_splits, paths)\n",
        "\n",
        "    # 13. Initialize the MERL model\n",
        "    model = MERL(config)\n",
        "\n",
        "    # 14. Visualize the model\n",
        "    model = visualize_model(model)\n",
        "\n",
        "    # 15. Test with dummy input\n",
        "    check_dummy_input(model, config['device'])\n",
        "\n",
        "    # 16. Define loss functions\n",
        "    cma_loss_fn, uma_loss_fn = define_loss_functions()\n",
        "\n",
        "    # 17. Train the model\n",
        "    model, train_losses = train_model(model, data_splits, config, paths)\n",
        "\n",
        "    # 18. Visualize embeddings with t-SNE\n",
        "    visualize_embeddings(model, data_splits, config, paths)\n",
        "\n",
        "    # 19. Evaluate model\n",
        "    eval_results = evaluate_model(model, data_splits, config, paths)\n",
        "\n",
        "    # 20. Perform zero-shot classification\n",
        "    zero_shot_results = perform_zero_shot(model, data_splits, config, paths)\n",
        "\n",
        "    # 21. Load pretrained model (in case we want to start from a checkpoint)\n",
        "    model = load_pretrained_model(model, paths)\n",
        "\n",
        "    # 22. Preprocess and generate memory for ECG signals\n",
        "    data_splits = preprocess_and_generate_memory(data_splits, paths)\n",
        "\n",
        "    # 23. Check for null values\n",
        "    data_splits = check_for_null_values(data_splits)\n",
        "\n",
        "    # 24. Generate embeddings\n",
        "    embeddings = generate_embeddings(model, data_splits, config, paths)\n",
        "\n",
        "    # 25. Generate category embeddings\n",
        "    category_embeddings = generate_category_embeddings(model, config, paths)\n",
        "    embeddings['category_embeddings'] = category_embeddings\n",
        "\n",
        "    # 26. Fine-tune the model using hierarchical loss function\n",
        "    model = fine_tune_model(model, data_splits, embeddings, config, paths)\n",
        "\n",
        "    # 27. Validate the fine-tuned model\n",
        "    validation_results = validate_model(model, data_splits, config, paths)\n",
        "\n",
        "    print(\"MERL model implementation complete!\")\n",
        "\n",
        "    # Report overall performance\n",
        "    print(\"\\nPerformance Summary:\")\n",
        "    print(f\"Zero-shot classification accuracy: {zero_shot_results.get('accuracy', 'N/A')}\")\n",
        "    print(f\"Fine-tuned model accuracy: {validation_results.get('accuracy', 'N/A') if validation_results else 'N/A'}\")\n",
        "\n",
        "# Run the main function if this script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "o8Hwid6RyOBC",
        "outputId": "7b9d2220-89b9-4c85-8b7a-d90f18780cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0e2079fb4bd7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;31m# Run the main function if this script is executed directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-0e2079fb4bd7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;34m'embedding_dim'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;34m'hierarchy_weight'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m     }\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    }
  ]
}